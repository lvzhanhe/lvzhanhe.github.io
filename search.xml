<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Web Crawler in Python]]></title>
    <url>%2F2017%2F08%2F07%2FWeb-Crawler-in-Python%2F</url>
    <content type="text"><![CDATA[Search engines like Google, Bin, and Yahoo have become an irreplaceable tool of human life today. They filter information and retrieve data, helping people find what they want. However, it is not so familiar to people how search engines work. Actually, the main part of them is web Crawler (also called Spider), which is heavily used in today’s technologies. They are used to get specific information on web pages and do further operations. Why python?As we can find, nowadays, most of web crawlers are based on Python. Lots of people may be curious about why it is Python. Actually, other languages like PHP, Java, C++, can also implement similar functions. However, Python has many advantages over them. First of all, Python has great HTTP libraries and HTML parsers. A basic one used to do this is “request”. An advanced one is “Beautiful Soup” (Mike 06), which is designed as a top of popular Python parsers like lxml. Furthermore, there are a lot of useful frameworks off the shelf. For instance, Tornado can support the I/O (input or output) from being blocked. Besides, Scrapy is a good one to build a scalable and distributed crawler. These will make developing period easier and more efficient. Therefore, it is safe to conclude that Python is a good choice for developing a web crawler. How does it work?The basic mechanism of web crawler is not complicated. Web crawler scans the web, reading specific data which has been set previously. It may start with some popular sites which have higher hits. Then it can spread through the internet by finding the related link within the sites. The crawler turns its finding to a giant index (Paul 05). In this giant index, it contains a great deal of data which meet requirements. A case in point, when a word “engineer” is entered in a search engine, it will check the whole index, then provide items related to “engineer”. Web crawler scan the web regularly to make sure its information is up to date. It sounds like an easy process. However, in order to acquire and then provide both accurate and reliable information, there are further steps to do (Vural 03). Web crawler should not only know what they get, but should also recognize where they get this data. It is extremely vital in practical application. For instance, a word appears in “heading” is more appropriate that which appears in “context”. For another, when one search in a video website, it will return particular information like name, singer, album. These are all related staff surrounding key words. Besides, there are always complicated algorithms based on the content of index. Essentially, these are equations to value and rate results coded by programmers. How to make a crawler?A python editor and a python environment are required to encode and implement a crawler program. Basically, the module called urllib is used to open a link in python program and return a HTML file.( Wael 03) The argument is just like page = urllib.request.urlopen(url, data, timeout). Initialing a variable for the link is recommended, since there are always further algorithms on it. The first parameter is the link, the second parameter is data need to be transported in this process, the final one is setting timeout. After that, the main task is to find useful information in that file. For example, specific data can be found by locating specific label name. The argument is like start_link = page.find(“&lt;span class = ‘title’&gt;”). Sometimes the data found just used to print in a python console. In order to be printed friendly, some small changes should be replaced. For instance, &lt;br&gt; should be replaced with &lt;\t&gt;. However, sometimes the data need to be stored in local file system by using the function urllib.urlretrieve() and a loop if necessary. In some dynamic websites, the data need to be transported to another page, like login page (Kristopher 05). At this point, there are two data transfer functions called get and post. For the function get, it transfers the data by creating a new url containing the data. And for the function post, it transfers by creating a dictionary containing different parameters. What’s it used for？Apart from the search engine mentioned in the beginning, there are a great number of projects can be achieved by using web crawler. There is an interesting example given by Emily L, a graduate student at Harvard University. Some programmers used web crawler to dig out what is the most common time for people to sleep. Since most people would like to post something like “I need to sleep” before they get to bed, the web crawler will scan through Twitter how many times the word “sleep” appeared in successive time intervals. Then it stores the data and presents it friendly as a diagram. Works citedThelwall, Mike. “A Web Crawler Design for DataMining.” vol. 27, SAGE Publications, Inc, Thousand Oaks, CA, 2011;2001;,doi:10.1177/016555150102700503. Gab–Allah, Wael A., Ben Bella S. Tawfik, and Hamed M.Nassar. “An Ontology Based Crawler for Retrieving Information Distributedon the Web.” International Journal of Engineering Research andApplications, vol. 6, no. 6, 2016, pp. 57-63. Jones, Kristopher B. Search Engine Optimization: Your VisualBlueprint for Effective Internet Marketing. Wiley, Indianapolis, IN, 2008. Zandbergen, Paul A. Python Scripting for ArcGIS. ESRI Press,New York, 2013;2015;.]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Crawler</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Useful Terminal Commands (1)]]></title>
    <url>%2F2017%2F07%2F19%2Fterminal1%2F</url>
    <content type="text"><![CDATA[Terminal provides a command line interface to control the UNIX-based operating system. Here’s something important you need to know about Terminal, and what it can do for you. pwdpwd is short for “print working directory”. It outputs the path of your current working directory. If you get lost, pwd can tell you where you are. 12➜ ~ git:(master) ✗ pwd/Users/LyuZhanhe lsThe ls command outputs the names of all of the folders and files in the working directory. ls is short for “list” (as in “list” the contents). Folder names end with a /, file names do not. Some terminals also add colorized output to ls to denote different types of files. For instance, Mimir’s terminal has folders in blue, and files in white. 1234➜ ~ git:(master) ✗ lsApplications Music PublicDesktop PhpstormProjects DownloadsDocuments Pictures iCloud Drive ls -l -R Desktop: List all the files on the desktop. -l, -R are parameters, and also can be written as -IR. -l means including details like permissions, owner, file size. -R means including its subdirectories. You can find more parameters here. ls d**: List all the files whose names begin with ‘d’. cdThe cd command is short for “change directory”. It allows you to change your working directory to some different folder. cd ../: Go up one level. 123➜ ~ git:(master) ✗ cd Desktop➜ Desktop git:(master) ✗ cd ../➜ ~ git:(master) ✗ mkdirmkdir [dir name]: Create a new directory. mkdir -p dir_1/dir_2: ‘p’ is short for parent. If dir_1 exists in your current working directory, create dir_2 in it. If not, create dir_1, then create dir_2 in it. 12345➜ Desktop git:(master) ✗ mkdir test➜ Desktop git:(master) ✗ mkdir test/test1➜ Desktop git:(master) ✗ cd test➜ test git:(master) ✗ lstest1 mkdir -m 777 dir_3: Create a directory named ‘dir_3’. It can be read, write and execute by any user. mkdir -m [xxx] dir_4: The first digit represents the owner, the second represents the group and the third represents other users. The number 7 represents all three types of permission (i.e., read, write and execute), 6 stands for read and write only, 5 is read and execute, 4 is read only, 3 is write and execute, 2 is write only, 1 is execute only and 0 is no permission. mkdir -v dir_5: Create a new directory. Also explain what it has done in the terminal, e.g. ‘mkdir: created directory ‘test3’’. 123➜ test git:(master) ✗ mkdir -m 343 test2➜ test git:(master) ✗ mkdir -v test3mkdir: created directory &apos;test3&apos; rmrm [file name]: Delete a file. You will be asked to confirm the operation. rm -f [file name]: Forcibly delete the file, the system no longer prompt. rm *.log: Delete all .log files, delete one by one before asking to confirm. 1234567891011➜ test git:(master) ✗ lsREADME.md README1.md README2.md README3.md test1 test2 test3➜ test git:(master) ✗ rm README.md➜ test git:(master) ✗ lsREADME1.md README2.md README3.md test1 test2 test3➜ test git:(master) ✗ rm -f README1.md➜ test git:(master) ✗ lsREADME2.md README3.md test1 test2 test3➜ test git:(master) ✗ rm *.md➜ test git:(master) ✗ lstest1 test2 test3 rmdirrmdir [dirname]: Remove directory (only operates on empty directories). rmdir -p [dirname]: When the subdirectory is deleted so that its parent has become an empty directory, then also delete the parent. 12345678➜ test git:(master) ✗ rmdir test1rmdir: test1: Directory not empty➜ test git:(master) ✗ rmdir test2➜ test git:(master) ✗ lstest1 test3➜ test git:(master) ✗ rmdir -p test3/test4➜ test git:(master) ✗ lstest1 mvmv [file] [new filename]: Rename a file. mv [file] [dirname]: Move a source file to a directory. mv [file1] [file2] [dirname]: Move a few files to a same directory. 1234567891011121314151617➜ test git:(master) ✗ ls -Rtest1 test2./test1:README.md README1.md README2.md./test2:➜ test git:(master) ✗ cd test1➜ test1 git:(master) ✗ mv README.md README1.md README2.md ../test2➜ test1 git:(master) ✗ cd ../➜ test git:(master) ✗ ls -Rtest1 test2./test1:./test2:README.md README1.md README2.md mv -i [file1] [file2]: Rename file1 to file2. If file2 doesn’t exist, prompt before overwriting. mv -f [file1] [file2]: Rename file1 to file2. Do not prompt before overwriting existing files. Please be careful when using it. mv [dir1] [dir2]: Move dir1 to dir2. If dir2 doesn’t exist, rename dir1 to dir2. mv * ../: Move all files under the current folder to the parent directory. mv [dir1]/* [dir2]: Move the files in dir1 to dir2.]]></content>
      <categories>
        <category>Terminal</category>
      </categories>
      <tags>
        <tag>Terminal</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F07%2F15%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Hi there! It’s Zhanhe :) WelcomeWelcome to my blog! This is my very first post. What is it?This blog is powered by Hexo and use the theme - NexT.Pisces. Generally, I will share what I am learning and researching or maybe what happened in my life. How to reach me?Feel free to share your feedback or ideas with me. Here’re some ways to reach me: Comment anything you want under a post. My email: zhanhe0827@gmail.com Github: https://github.com/lvzhanhe]]></content>
      <tags>
        <tag>About</tag>
      </tags>
  </entry>
</search>