<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Web Crawler in Python]]></title>
    <url>%2F2017%2F08%2F07%2FWeb-Crawler-in-Python%2F</url>
    <content type="text"><![CDATA[Search engines like Google,Bin, and Yahoo have become an irreplaceable tool of human life today. They filter information and retrieve data, helping people find what they want. However, it is not so familiar to people how search engines work. Actually, the main part of them is web Crawler (also called Spider), which is heavily used in today’s technologies. They are used to get specific information on web pages and do further operations. Why python?As we can find, nowadays, most of web crawlers are based on Python. Lots of people may be curious about why it is Python. Actually, other languages like PHP, Java, C++, can also implement similar functions. However, Python has many advantages over them. First of all, Python has great HTTP libraries and HTML parsers. A basic one used to do this is “request”. An advanced one is “Beautiful Soup” (Mike 06), which is designed as a top of popular Python parsers like lxml. Furthermore, there are a lot of useful frameworks off the shelf. For instance, Tornado can support the I/O (input or output) from being blocked. Besides, Scrapy is a good one to build a scalable and distributed crawler. These will make developing period easier and more efficient. Therefore, it is safe to conclude that Python is a good choice for developing a web crawler. How it works?The basic mechanism of web crawler is not complicated. Web crawler scan the web, reading specific data which has been set previously. It may start with some popular sites which have higher hits. Then it can spread through the internet by finding the related link within the sites. The crawler turns its finding to a giant index (Paul 05). In this giant index, it contains a great deal of data which meets requirements. A case in point, when a word “engineer” is entered in a search engine, it will check the whole index, then provide items related to “engineer”. Web crawler scan the web regularly to make sure its information is up to date. It sounds like an easy process. However, in order to acquire and then provide both accurate and reliable information, there are further steps to do (Vural 03). Web crawler should not only know what they get, but should also recognize where they get this data. It is extremely vital in practical application. For instance, a word appears in “heading” is more appropriate that which appear in “context”. For another, when one search in a video website, it will return particular information like name, singer, album. These are all related staff surrounding key words. Besides, there are always complicated algorithms based on the content of index. Essentially, these are equations to value and rate results coded by programmers. How to make a crawler?A python editor and a python environment are required to encode and implement a crawler program. Basically, the module called “urllib” is used to open a link in python program and return a HTML file.( Wael 03) The argument is just like “page = urllib.request.urlopen(url, data, timeout)”. Initialing a variable for the link (just like ‘page’) is recommended, since there are always further algorithms on it. The first parameter is the link, the second parameter is data need to be transported in this process, the final one is setting timeout. After that, the main task is to find useful information in that file. For example, specific data can be found by locating specific label name. The argument is like “start_link = page.find(“”)”. Sometimes the data found just used to print in a python console. In order to be printed friendly, some small changes should be replaced. For instance, “” should be replaced with “&lt;\t&gt;” and “ ” should be replaced with a space. However, sometimes the data need to be stored in local file system by using the function “urllib.urlretrieve()” and a loop if necessary. In some dynamic websites, the data need to be transported to another page, like log in page (Kristopher 05). At this point, there are two data transfer functions called get and post. For the function “get”, it transfers the data by creating a new url containing the data. And for the function “post”, it transfers by creating a dictionary containing different parameters. What‘s it used for？Apart from the search engine mentioned in the beginning, there are a great number of projects can be achieved by using web crawler. There is an interesting example given by Emily L, a graduate studentin Harvard University. Some programmers used web crawler to dig out what is the most common time for people to sleep. Since most people would like to post something like “I need to sleep” before they get to bed, the web crawler will scan through Twitter how many times the word “sleep” was appeared in successive time intervals. Then it stores the data and present it friendly as a diagram. Works citedThelwall, Mike. “A Web Crawler Design for DataMining.” vol. 27, SAGE Publications, Inc, Thousand Oaks, CA, 2011;2001;,doi:10.1177/016555150102700503. Gab–Allah, Wael A., Ben Bella S. Tawfik, and Hamed M.Nassar. “An Ontology Based Crawler for Retrieving Information Distributedon the Web.” International Journal of Engineering Research andApplications, vol. 6, no. 6, 2016, pp. 57-63. Jones, Kristopher B. Search Engine Optimization: Your VisualBlueprint for Effective Internet Marketing. Wiley, Indianapolis, IN, 2008. Zandbergen, Paul A. Python Scripting for ArcGIS. ESRI Press,New York, 2013;2015;. Vural, A. G., B. B. Cambazoglu, and Pinar Karagoz.”Sentiment-Focused Web Crawling.” ACM Transactions on the Web (TWEB),vol. 8, no. 4, 2014, pp. 1-21, doi:10.1145/2644821.]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Crawler</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Useful Terminal Commands]]></title>
    <url>%2F2017%2F07%2F19%2Fterminal1%2F</url>
    <content type="text"><![CDATA[Terminal provides a command line interface to control the UNIX-based operating system. Here’s something important you need to know about Terminal, and what it can do for you. pwdpwd is short for “print working directory”. It outputs the path of your current working directory. If you get lost, pwd can tell you where you are. lsThe ls command outputs the names of all of the folders and files in the working directory. ls is short for “list” (as in “list” the contents). Folder names end with a /, file names do not. Some terminals also add colorized output to ls to denote different types of files. For instance, Mimir’s terminal has folders in blue, and files in white. cdThe cd command is short for “change directory”. It allows you to change your working directory to some different folder. To be continued…]]></content>
      <categories>
        <category>Terminal</category>
      </categories>
      <tags>
        <tag>Terminal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F07%2F15%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Hi there! It’s Zhanhe :) WelcomeWelcome to my blog! This is my very first post. What is it?It’s super exciting to talk to you here. This blog is powered by Hexo and use the theme - NexT.Pisces. Generally, I will share what I am learning and researching or maybe what happened in my life. How to reach me?Feel free to share your feedback or ideas with me. Here’re some ways to reach me: Comment anything you want under a post. My email: zhanhe0827@gmail.com Github: https://github.com/lvzhanhe]]></content>
      <tags>
        <tag>About</tag>
      </tags>
  </entry>
</search>